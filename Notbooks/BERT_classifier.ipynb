{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e5ff48b",
   "metadata": {},
   "source": [
    "# **Text mining: SENTIMENT ANALYSIS**\n",
    "\n",
    "## ðŸŽ“ Masterâ€™s Program in Data Science & Advanced Analytics  \n",
    "**Nova IMS** | March 2025  \n",
    "**Course:** Business Cases with Data Science\n",
    "\n",
    "## ðŸ‘¥ Team **Group 34**  \n",
    "- **[Philippe Dutranoit]** | [20240518]  \n",
    "- **[Diogo Duarte]** | [20240525]  \n",
    "- **[Rui luz]** | [20211628]  \n",
    "- **[Rodrigo Sardinha]** | [20211627]  \n",
    "\n",
    "## ðŸ“Š Goal of the notebook\n",
    "\n",
    "This notebook focuses on feature selection and engineering for our text-mining project: predicting market sentiment (Bearish, Bullish, Neutral) from Twitter data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f052f0b3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d0d660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruibl\\anaconda3\\envs\\nel_2025_env_full\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d424eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../Data/X_train.csv')\n",
    "y_train = pd.read_csv('../Data/y_train.csv')\n",
    "X_test = pd.read_csv('../Data/X_val.csv')\n",
    "y_test = pd.read_csv('../Data/y_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc46e035",
   "metadata": {},
   "source": [
    "# Prepocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927c41da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model \n",
    "MODEL_NAME = 'cardiffnlp/twitter-roberta-base-sentiment-latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e18b94",
   "metadata": {},
   "source": [
    "## Preprocessing with a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff3d87a",
   "metadata": {},
   "source": [
    "### Define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53dc7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruibl\\anaconda3\\envs\\nel_2025_env_full\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b903506",
   "metadata": {},
   "source": [
    "### Encode the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc1a7b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize training set\n",
    "train_encodings = tokenizer(\n",
    "    X_train[\"text\"].tolist(),   # convert to list of strings\n",
    "    padding=True,               # pad to max length in batch\n",
    "    truncation=True,            # truncate long tweets\n",
    "    return_tensors=\"pt\"         # return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ff8f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize validation set\n",
    "val_encodings = tokenizer(\n",
    "    X_test[\"text\"].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e210307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input_ids shape: torch.Size([7634, 105])\n",
      "Validation input_ids shape: torch.Size([1909, 83])\n"
     ]
    }
   ],
   "source": [
    "# Check shape of tokenized data\n",
    "print(\"Train input_ids shape:\", train_encodings[\"input_ids\"].shape)\n",
    "print(\"Validation input_ids shape:\", val_encodings[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e8a7c",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7595e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for our BERT model\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce5ea0",
   "metadata": {},
   "source": [
    "#### Instantiate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35a3eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to list\n",
    "train_labels = y_train[\"label\"].tolist()\n",
    "val_labels = y_test[\"label\"].tolist()\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "val_dataset = TweetDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b63e40",
   "metadata": {},
   "source": [
    "#### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc64737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size (typical: 16 or 32)\n",
    "batch_size = 16\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c7b474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "torch.Size([16, 105])\n",
      "torch.Size([16, 105])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Check one batch from train_loader\n",
    "batch = next(iter(train_loader))\n",
    "print(batch.keys())\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['attention_mask'].shape)\n",
    "print(batch['labels'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cb6dde",
   "metadata": {},
   "source": [
    "# Model prep "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e526d3e0",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04bc658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model with 3 output labels\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1443ed8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20c50d",
   "metadata": {},
   "source": [
    "## Setup model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6597d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f06bf5c7",
   "metadata": {},
   "source": [
    "# Model Training "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nel_2025_env_full",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
